import pandas as pd
import numpy as np
from unidecode import unidecode
import re
from typing import List, Dict, Tuple, Optional
import colorsys
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import SnowballStemmer
import warnings
warnings.filterwarnings('ignore')

class DuplicateDetector:
    def __init__(self, similarity_threshold: float = 0.85):
        """
        –î–µ—Ç–µ–∫—Ç–æ—Ä –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –∏ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
        
        Args:
            similarity_threshold: –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –æ—Ç 0 –¥–æ 1 (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.85)
        """
        self.similarity_threshold = similarity_threshold
        self.vectorizer = None
        self.stemmer = SnowballStemmer('russian')
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLTK –¥–∞–Ω–Ω—ã—Ö
        self._init_nltk()
        
        # –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
        self.common_words = {
            'org_forms': r'\b(–∏–ø|–æ–æ–æ|–æ–∞–æ|–∑–∞–æ|—Ç–æ–≤|ltd|llc|inc|corporation|corp|company|co)\b',
            'venue_types': r'\b(–∫–∞—Ñ–µ|—Ä–µ—Å—Ç–æ—Ä–∞–Ω|–±–∞—Ä|—Å—Ç–æ–ª–æ–≤–∞—è|–º–∞–≥–∞–∑–∏–Ω|—Ç–æ—Ä–≥–æ–≤–∞—è —Ç–æ—á–∫–∞|—Ç—Ç|cafe|restaurant|bar|shop|store)\b',
            'punctuation': r'[.,;:!?()"\'\-\‚Ññ#@$%^&*+={}|\\`~<>/]',
            'extra_spaces': r'\s+',
            'postal_codes': r'^\d{5,6},?\s*',
            'numbers': r'\d+',
            'address_abbreviations': {
                r'\b–≥\b': '–≥–æ—Ä–æ–¥',
                r'\b—É–ª\b': '—É–ª–∏—Ü–∞', 
                r'\b–ø—Ä\b': '–ø—Ä–æ—Å–ø–µ–∫—Ç',
                r'\b–¥\b': '–¥–æ–º',
                r'\b—Å—Ç—Ä\b': '—Å—Ç—Ä–æ–µ–Ω–∏–µ',
                r'\b–∫\b': '–∫–æ—Ä–ø—É—Å',
                r'\b–æ–±–ª\b': '–æ–±–ª–∞—Å—Ç—å',
                r'\b—Ä-–Ω\b': '—Ä–∞–π–æ–Ω',
                r'\b–ø–æ—Å\b': '–ø–æ—Å–µ–ª–æ–∫',
                r'\b—Å\b': '—Å–µ–ª–æ',
                r'\b–¥–µ—Ä\b': '–¥–µ—Ä–µ–≤–Ω—è',
                r'\b–ø–ª\b': '–ø–ª–æ—â–∞–¥—å',
                r'\b–ø–µ—Ä\b': '–ø–µ—Ä–µ—É–ª–æ–∫',
                r'\b—à\b': '—à–æ—Å—Å–µ',
                r'\b–Ω–∞–±\b': '–Ω–∞–±–µ—Ä–µ–∂–Ω–∞—è'
            }
        }
        
        # –†—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        self.russian_stopwords = {
            '–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–∑–∞', '–æ—Ç', '–¥–æ', '–¥–ª—è', '–ø—Ä–∏', '–æ', 
            '–æ–±', '–ø–æ–¥', '–Ω–∞–¥', '–º–µ–∂–¥—É', '—á–µ—Ä–µ–∑', '–±–µ–∑', '–∏–∑', '–∫', '—É', '—Ç',
            '–Ω–æ', '–∞', '—á—Ç–æ', '–∫–∞–∫', '–Ω–µ', '–∂–µ', '–ª–∏', '–±—ã', '–¥–∞', '–∏–ª–∏'
        }
    
    def _init_nltk(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLTK –¥–∞–Ω–Ω—ã—Ö"""
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            nltk.download('punkt', quiet=True)
        
        try:
            nltk.data.find('corpora/stopwords')
        except LookupError:
            nltk.download('stopwords', quiet=True)

    def generate_colors(self, num_colors: int, is_dark_theme: bool = False) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ü–≤–µ—Ç–æ–≤ –¥–ª—è –≥—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        colors = []
        for i in range(num_colors):
            hue = i / num_colors
            
            if is_dark_theme:
                saturation = 0.6 + (i % 3) * 0.1
                lightness = 0.4 + (i % 4) * 0.1
            else:
                saturation = 0.4 + (i % 3) * 0.1
                lightness = 0.85 + (i % 3) * 0.05
            
            rgb = colorsys.hls_to_rgb(hue, lightness, saturation)
            hex_color = "#{:02x}{:02x}{:02x}".format(int(rgb[0]*255), int(rgb[1]*255), int(rgb[2]*255))
            colors.append(hex_color)
        return colors

    def preprocess_text(self, text: str, remove_org_forms: bool = True, remove_venue_types: bool = True) -> str:
        """
        –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        
        Args:
            text: –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            remove_org_forms: —É–±–∏—Ä–∞—Ç—å –ª–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω–æ-–ø—Ä–∞–≤–æ–≤—ã–µ —Ñ–æ—Ä–º—ã
            remove_venue_types: —É–±–∏—Ä–∞—Ç—å –ª–∏ —Ç–∏–ø—ã –∑–∞–≤–µ–¥–µ–Ω–∏–π
            
        Returns:
            –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        if pd.isna(text) or not text:
            return ""
        
        text = str(text).strip().lower()
        
        # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
        text = re.sub(self.common_words['punctuation'], ' ', text)
        
        # –£–¥–∞–ª—è–µ–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω–æ-–ø—Ä–∞–≤–æ–≤—ã–µ —Ñ–æ—Ä–º—ã
        if remove_org_forms:
            text = re.sub(self.common_words['org_forms'], '', text)
        
        # –£–¥–∞–ª—è–µ–º —Ç–∏–ø—ã –∑–∞–≤–µ–¥–µ–Ω–∏–π
        if remove_venue_types:
            text = re.sub(self.common_words['venue_types'], '', text)
        
        # –£–¥–∞–ª—è–µ–º —á–∏—Å–ª–∞ (–Ω–æ–º–µ—Ä–∞ –¥–æ–º–æ–≤, —Ç–µ–ª–µ—Ñ–æ–Ω—ã –∏ —Ç.–¥.)
        text = re.sub(self.common_words['numbers'], '', text)
        
        # –¢—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è
        try:
            text = unidecode(text)
        except:
            pass
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        try:
            tokens = word_tokenize(text, language='russian')
        except:
            tokens = text.split()
        
        # –£–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
        tokens = [token for token in tokens 
                 if len(token) > 2 and token not in self.russian_stopwords]
        
        # –°—Ç–µ–º–º–∏–Ω–≥
        try:
            tokens = [self.stemmer.stem(token) for token in tokens]
        except:
            pass
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        result = ' '.join(tokens)
        result = re.sub(self.common_words['extra_spaces'], ' ', result).strip()
        
        return result

    def normalize_address(self, address: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∞–¥—Ä–µ—Å–∞"""
        if pd.isna(address) or not address:
            return ""
        
        address = str(address).lower().strip()
        
        # –£–±–∏—Ä–∞–µ–º –ø–æ—á—Ç–æ–≤—ã–µ –∫–æ–¥—ã
        address = re.sub(self.common_words['postal_codes'], '', address)
        
        # –ó–∞–º–µ–Ω—è–µ–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è
        for pattern, replacement in self.common_words['address_abbreviations'].items():
            address = re.sub(pattern, replacement, address)
        
        # –£–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –Ω–æ–º–µ—Ä–∞
        address = re.sub(r'[,.\-‚Ññ]', ' ', address)
        address = re.sub(self.common_words['numbers'], '', address)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        address = re.sub(self.common_words['extra_spaces'], ' ', address).strip()
        
        return address

    def create_combined_features(self, df: pd.DataFrame, name_column: str, address_column: Optional[str] = None) -> List[str]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        
        Args:
            df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            name_column: –∫–æ–ª–æ–Ω–∫–∞ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏
            address_column: –∫–æ–ª–æ–Ω–∫–∞ —Å –∞–¥—Ä–µ—Å–∞–º–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            —Å–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        """
        combined_features = []
        
        for idx, row in df.iterrows():
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
            name_text = self.preprocess_text(row[name_column])
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∞–¥—Ä–µ—Å, –µ—Å–ª–∏ –µ—Å—Ç—å
            address_text = ""
            if address_column and address_column in df.columns:
                address_text = self.normalize_address(row[address_column])
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ –∞–¥—Ä–µ—Å —Å –≤–µ—Å–∞–º–∏
            # –ù–∞–∑–≤–∞–Ω–∏–µ –≤–∞–∂–Ω–µ–µ –∞–¥—Ä–µ—Å–∞, –ø–æ—ç—Ç–æ–º—É –¥—É–±–ª–∏—Ä—É–µ–º –µ–≥–æ
            combined_text = f"{name_text} {name_text} {address_text}".strip()
            combined_features.append(combined_text)
        
        return combined_features

    def calculate_similarity_matrix(self, texts: List[str]) -> np.ndarray:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã —Å—Ö–æ–∂–µ—Å—Ç–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TF-IDF –∏ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
        
        Args:
            texts: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            
        Returns:
            –º–∞—Ç—Ä–∏—Ü–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏
        """
        # –°–æ–∑–¥–∞–µ–º TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä
        self.vectorizer = TfidfVectorizer(
            max_features=10000,
            ngram_range=(1, 2),  # –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏–≥—Ä–∞–º–º—ã –∏ –±–∏–≥—Ä–∞–º–º—ã
            min_df=1,
            max_df=0.95,
            lowercase=True,
            analyzer='word'
        )
        
        # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç—ã
        tfidf_matrix = self.vectorizer.fit_transform(texts)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarity_matrix = cosine_similarity(tfidf_matrix)
        
        return similarity_matrix

    def find_duplicates(self, df: pd.DataFrame, name_column: str, address_column: Optional[str] = None, id_column: Optional[str] = None) -> Tuple[List[List[int]], Dict]:
        """
        –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        
        Args:
            df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            name_column: –∫–æ–ª–æ–Ω–∫–∞ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏
            address_column: –∫–æ–ª–æ–Ω–∫–∞ —Å –∞–¥—Ä–µ—Å–∞–º–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            id_column: –∫–æ–ª–æ–Ω–∫–∞ —Å ID (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
            
        Returns:
            (–≥—Ä—É–ø–ø—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞)
        """
        if df.empty:
            return [], {'total_records': 0, 'duplicate_groups': 0, 'duplicate_records': 0, 'unique_records': 0}
        
        # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        combined_features = self.create_combined_features(df, name_column, address_column)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã
        valid_indices = [i for i, text in enumerate(combined_features) if text.strip()]
        
        if not valid_indices:
            return [], {'total_records': len(df), 'duplicate_groups': 0, 'duplicate_records': 0, 'unique_records': len(df)}
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
        valid_texts = [combined_features[i] for i in valid_indices]
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–∞—Ç—Ä–∏—Ü—É —Å—Ö–æ–∂–µ—Å—Ç–∏
        similarity_matrix = self.calculate_similarity_matrix(valid_texts)
        
        # –ù–∞—Ö–æ–¥–∏–º –≥—Ä—É–ø–ø—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        duplicate_groups = []
        processed_indices = set()
        
        for i, original_idx in enumerate(valid_indices):
            if original_idx in processed_indices:
                continue
                
            current_group = [original_idx]
            
            for j, compare_idx in enumerate(valid_indices[i+1:], i+1):
                if compare_idx in processed_indices:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
                similarity = similarity_matrix[i][j]
                
                if similarity >= self.similarity_threshold:
                    current_group.append(compare_idx)
            
            if len(current_group) > 1:
                duplicate_groups.append(current_group)
                processed_indices.update(current_group)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = {
            'total_records': len(df),
            'duplicate_groups': len(duplicate_groups),
            'duplicate_records': sum(len(group) for group in duplicate_groups),
            'unique_records': len(df) - sum(len(group) for group in duplicate_groups)
        }
        
        return duplicate_groups, stats

    def create_grouped_dataframe(self, df: pd.DataFrame, duplicate_groups: List[List[int]]) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ DataFrame —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        if not duplicate_groups:
            result_df = df.copy()
            result_df['Duplicate_Group'] = 0
            return result_df
        
        grouped_data = []
        group_counter = 1
        
        for group in duplicate_groups:
            for row_idx in group:
                row_data = df.iloc[row_idx].to_dict()
                row_data['Duplicate_Group'] = group_counter
                grouped_data.append(row_data)
            group_counter += 1
        
        # –î–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–∞–ø–∏—Å–∏
        remaining_indices = set(range(len(df))) - set(idx for group in duplicate_groups for idx in group)
        for row_idx in remaining_indices:
            row_data = df.iloc[row_idx].to_dict()
            row_data['Duplicate_Group'] = 0
            grouped_data.append(row_data)
        
        result_df = pd.DataFrame(grouped_data)
        result_df = result_df.sort_values(['Duplicate_Group', result_df.columns[0]], ascending=[False, True])
        
        return result_df

    def create_styled_dataframe(self, df: pd.DataFrame, duplicate_groups: List[List[int]], is_dark_theme: bool = False) -> Dict:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç–∏–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ DataFrame –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        grouped_df = self.create_grouped_dataframe(df, duplicate_groups)
        
        if not duplicate_groups:
            return {
                "data": grouped_df.values.tolist(),
                "headers": grouped_df.columns.tolist(),
                "metadata": {"styling": []},
            }
        
        colors = self.generate_colors(len(duplicate_groups), is_dark_theme)
        
        styling = []
        for _ in range(len(grouped_df)):
            styling.append([""] * len(grouped_df.columns))
        
        for idx, row in grouped_df.iterrows():
            group_num = row['Duplicate_Group']
            if group_num > 0:
                color = colors[(group_num - 1) % len(colors)]
                for col_idx in range(len(grouped_df.columns)):
                    styling[idx][col_idx] = f"background-color: {color};"
        
        return {
            "data": grouped_df.values.tolist(),
            "headers": grouped_df.columns.tolist(),
            "metadata": {
                "styling": styling,
            },
        }

    def find_name_column(self, df: pd.DataFrame) -> Optional[str]:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏—Ç –∫–æ–ª–æ–Ω–∫—É —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏/–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è–º–∏"""
        columns = [col.lower() for col in df.columns]
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–ª–æ–Ω–∫–∏ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏
        name_keywords = [
            '–Ω–∞–∑–≤–∞–Ω', '–Ω–∞–∏–º–µ–Ω', '–∏–º—è', 'name', 'title', '–∫–æ–º–ø–∞–Ω', '–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü', 
            '–ø—Ä–µ–¥–ø—Ä–∏—è—Ç', '—Ñ–∏—Ä–º', 'company', 'organization', 'firm'
        ]
        
        for i, col in enumerate(columns):
            for keyword in name_keywords:
                if keyword in col:
                    return df.columns[i]
        
        return None
    
    def find_address_column(self, df: pd.DataFrame) -> Optional[str]:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏—Ç –∫–æ–ª–æ–Ω–∫—É —Å –∞–¥—Ä–µ—Å–∞–º–∏"""
        columns = [col.lower() for col in df.columns]
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–ª–æ–Ω–∫–∏ —Å –∞–¥—Ä–µ—Å–∞–º–∏
        address_keywords = ['–∞–¥—Ä–µ—Å', 'address', '–º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω', 'location']
        
        for i, col in enumerate(columns):
            for keyword in address_keywords:
                if keyword in col:
                    return df.columns[i]
        
        return None

    def get_similarity_info(self) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–µ—Ç–æ–¥–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ö–æ–∂–µ—Å—Ç–∏"""
        return f"""
        üî¨ **–ú–µ—Ç–æ–¥ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤:**
        - –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Å TF-IDF
        - –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
        - –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏: {self.similarity_threshold:.2f}
        - –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞: —Å—Ç–µ–º–º–∏–Ω–≥, —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤
        """ 