import pandas as pd
import re
from thefuzz import fuzz
import networkx as nx
from collections import Counter
import colorsys
from typing import List, Dict, Tuple, Optional
import warnings
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer

warnings.filterwarnings("ignore")


class DuplicateDetector:
    def __init__(self, similarity_threshold: float = 0.80):
        self.similarity_threshold = similarity_threshold
        self._setup_nltk()

    def _setup_nltk(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLTK —Ä–µ—Å—É—Ä—Å–æ–≤"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Ä—É—Å—Å–∫–∏—Ö —Å—Ç–æ–ø-—Å–ª–æ–≤
            self.russian_stopwords = set(stopwords.words('russian'))
            self.stemmer = SnowballStemmer('russian')
            self.use_nltk = True
        except Exception as e:
            print(f"‚ö†Ô∏è NLTK resources not available: {e}")
            self.russian_stopwords = set()
            self.stemmer = None
            self.use_nltk = False

    def normalize_text(self, text, sort_words=True):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º NLTK"""
        if not isinstance(text, str): 
            return ""
            
        text = text.lower()
        
        # –ë–∞–∑–æ–≤—ã–µ –∑–∞–º–µ–Ω—ã –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        replacements = {
            r'\b–æ–±—â–µ—Å—Ç–≤–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å—é\b': '–æ–æ–æ', 
            r'(\b–∏–ø\b)|(\b–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å\b)': ' ',
            r'\b–≥–æ—Ä–æ–¥\b': '–≥', 
            r'\b—É–ª–∏—Ü–∞\b': '—É–ª', 
            r'\b–¥–æ–º\b': '–¥', 
            r'\b—Å—Ç—Ä–æ–µ–Ω–∏–µ\b': '—Å—Ç—Ä', 
            r'\b–∫–æ—Ä–ø—É—Å\b': '–∫',
        }
        for p, r in replacements.items(): 
            text = re.sub(p, r, text)
        
        if self.use_nltk:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º NLTK –¥–ª—è –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            try:
                # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é NLTK (–ª—É—á—à–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é)
                tokens = word_tokenize(text, language='russian')
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã –∏ —Ü–∏—Ñ—Ä—ã
                tokens = [token for token in tokens if re.match(r'^[a-z–∞-—è0-9]+$', token)]
                
                # –£–¥–∞–ª—è–µ–º —Ä—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
                tokens = [token for token in tokens if token not in self.russian_stopwords]
                
                # –°—Ç–µ–º–º–∏–Ω–≥ –¥–ª—è –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è –∫ –∫–æ—Ä–Ω–µ–≤–æ–π —Ñ–æ—Ä–º–µ
                if self.stemmer:
                    tokens = [self.stemmer.stem(token) for token in tokens]
                
                words = tokens
            except Exception as e:
                # Fallback –∫ –±–∞–∑–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ NLTK
                text = re.sub(r'[^a-z–∞-—è0-9\s]', ' ', text)
                words = text.split()
        else:
            # –ë–∞–∑–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ NLTK
            text = re.sub(r'[^a-z–∞-—è0-9\s]', ' ', text)
            words = text.split()
        
        # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ (–º–µ–Ω–µ–µ 2 —Å–∏–º–≤–æ–ª–æ–≤)
        words = [word for word in words if len(word.strip()) >= 2]
        
        if sort_words: 
            words = sorted(words)
        return ' '.join(words).strip()

    def normalize_brand_name(self, text):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –±—Ä–µ–Ω–¥–æ–≤–æ–≥–æ –∏–º–µ–Ω–∏ —Å NLTK"""
        base_normalized = self.normalize_text(text, sort_words=False)
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ç–∏–ø–æ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        generic_words = {
            '–æ–æ–æ', '–∫–∞—Ñ–µ', '–±–∞—Ä', '—Ä–µ—Å—Ç–æ—Ä–∞–Ω', '—Ñ–∞—Å—Ç—Ñ—É–¥', '–º–∞–≥–∞–∑–∏–Ω', '–ø–∏—Ü—Ü–µ—Ä–∏—è', 
            '–∫–ª—É–±', '–∫–æ—Ñ–µ–π–Ω—è', '—Å—Ç–æ–ª–æ–≤', '–µ–¥', '—Ç–æ—Ä–≥–æ–≤', '—Ü–µ–Ω—Ç—Ä', '–¥–æ–º', '–∫–æ–º–ø–∞–Ω',
            '–ø—Ä–µ–¥–ø—Ä–∏—è—Ç', '–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω', '–æ–±—â–µ—ç—Å—Ç–≤', '–æ–≥—Ä–∞–Ω–∏—á–µ–Ω', '–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç'
        }
        
        words = [word for word in base_normalized.split() if word not in generic_words]
        return ' '.join(sorted(words))

    def normalize_text_gentle(self, text):
        """–ú—è–≥–∫–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        if not isinstance(text, str): 
            return ""
            
        text = text.lower().strip()
        
        # –¢–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã–µ –∑–∞–º–µ–Ω—ã
        replacements = {
            r'\b–æ–±—â–µ—Å—Ç–≤–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å—é\b': '–æ–æ–æ', 
            r'(\b–∏–ø\b)|(\b–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å\b)': '–∏–ø',
            r'\b–≥–æ—Ä–æ–¥\b': '–≥', 
            r'\b—É–ª–∏—Ü–∞\b': '—É–ª', 
            r'\b–¥–æ–º\b': '–¥', 
            r'\b—Å—Ç—Ä–æ–µ–Ω–∏–µ\b': '—Å—Ç—Ä', 
            r'\b–∫–æ—Ä–ø—É—Å\b': '–∫',
        }
        for p, r in replacements.items(): 
            text = re.sub(p, r, text)
        
        # –£–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —è–≤–Ω—É—é –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()

    # ==========================================================================
    # –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ß–ï–¢–´–†–ï–• –ù–ï–ó–ê–í–ò–°–ò–ú–´–• –°–£–î–ï–ô
    # ==========================================================================

    def get_duplicates_judge1_strict(self, df):
        """–°—É–¥—å—è 1: '–°—Ç—Ä–æ–≥–∏–π –ö–æ–Ω—Ç—Ä–æ–ª—å' - –ø–æ–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥–∏ –¥–ª—è –±–æ–ª—å—à–µ–π —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."""
        duplicates = set()
        ids = df.index.tolist()
        for i in range(len(ids)):
            for j in range(i + 1, len(ids)):
                id1, id2 = ids[i], ids[j]
                name_sim = fuzz.token_set_ratio(df.loc[id1, 'norm_name'], df.loc[id2, 'norm_name'])
                addr_sim = fuzz.token_set_ratio(df.loc[id1, 'norm_addr'], df.loc[id2, 'norm_addr'])
                # –ü–æ–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥–∏: –±—ã–ª–æ 95/90, —Å—Ç–∞–ª–æ 85/80
                if addr_sim > 85 and name_sim > 80:
                    duplicates.add(frozenset([id1, id2]))
        return duplicates

    def get_duplicates_judge2_geo(self, df):
        """–°—É–¥—å—è 2: '–ì–µ–æ-–ê–Ω–∞–ª–∏—Ç–∏–∫' - –ø–æ–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥–∏."""
        duplicates = set()
        ids = df.index.tolist()
        for i in range(len(ids)):
            for j in range(i + 1, len(ids)):
                id1, id2 = ids[i], ids[j]
                addr_sim = fuzz.token_set_ratio(df.loc[id1, 'norm_addr'], df.loc[id2, 'norm_addr'])
                # –ü–æ–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥–∏: –±—ã–ª–æ 97/70, —Å—Ç–∞–ª–æ 88/60 (–≤–∫–ª—é—á–∞–µ–º –≥—Ä–∞–Ω–∏—á–Ω—ã–µ —Å–ª—É—á–∞–∏)
                if addr_sim >= 88:
                    name_sim = fuzz.token_set_ratio(df.loc[id1, 'norm_name'], df.loc[id2, 'norm_name'])
                    if name_sim > 60:
                        duplicates.add(frozenset([id1, id2]))
        return duplicates

    def get_duplicates_judge3_brand(self, df):
        """–°—É–¥—å—è 3: '–ê–Ω–∞–ª–∏–∑ –ø–æ –ë—Ä–µ–Ω–¥—É' - –ø–æ–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥–∏."""
        duplicates = set()
        ids = df.index.tolist()
        for i in range(len(ids)):
            for j in range(i + 1, len(ids)):
                id1, id2 = ids[i], ids[j]
                brand_name_sim = fuzz.token_set_ratio(df.loc[id1, 'brand_name'], df.loc[id2, 'brand_name'])
                # –ü–æ–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥–∏: –±—ã–ª–æ 95/75, —Å—Ç–∞–ª–æ 85/65
                if brand_name_sim > 85:
                    addr_sim = fuzz.token_set_ratio(df.loc[id1, 'norm_addr'], df.loc[id2, 'norm_addr'])
                    if addr_sim > 65:
                        duplicates.add(frozenset([id1, id2]))
        return duplicates

    def get_duplicates_judge4_weighted(self, df):
        """–°—É–¥—å—è 4: '–ò–Ω—Ç–µ–≥—Ä–∞—Ç–æ—Ä' (–í–∑–≤–µ—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞) - –ø–æ–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥."""
        duplicates = set()
        SIMILARITY_THRESHOLD = 78  # –ü–æ–Ω–∏–∂–∞–µ–º: –±—ã–ª–æ 88, —Å—Ç–∞–ª–æ 78
        NAME_WEIGHT = 0.45
        ADDRESS_WEIGHT = 0.55
        ids = df.index.tolist()
        for i in range(len(ids)):
            for j in range(i + 1, len(ids)):
                id1, id2 = ids[i], ids[j]
                name_sim = fuzz.token_set_ratio(df.loc[id1, 'norm_name'], df.loc[id2, 'norm_name'])
                addr_sim = fuzz.token_set_ratio(df.loc[id1, 'norm_addr'], df.loc[id2, 'norm_addr'])
                weighted_similarity = (name_sim * NAME_WEIGHT) + (addr_sim * ADDRESS_WEIGHT)
                if weighted_similarity >= SIMILARITY_THRESHOLD:
                    duplicates.add(frozenset([id1, id2]))
        return duplicates

    def get_duplicates_judge5_liberal(self, df):
        """–°—É–¥—å—è 5: '–õ–∏–±–µ—Ä–∞–ª—å–Ω—ã–π' - –æ—á–µ–Ω—å –º—è–≥–∫–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –æ—á–µ–≤–∏–¥–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤."""
        duplicates = set()
        ids = df.index.tolist()
        for i in range(len(ids)):
            for j in range(i + 1, len(ids)):
                id1, id2 = ids[i], ids[j]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –±–µ–∑ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
                orig_name1 = str(df.loc[id1, 'orig_name']) if 'orig_name' in df.columns else ""
                orig_name2 = str(df.loc[id2, 'orig_name']) if 'orig_name' in df.columns else ""
                orig_addr1 = str(df.loc[id1, 'orig_addr']) if 'orig_addr' in df.columns else ""
                orig_addr2 = str(df.loc[id2, 'orig_addr']) if 'orig_addr' in df.columns else ""
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                orig_name_ratio = fuzz.ratio(orig_name1.lower(), orig_name2.lower())
                orig_addr_ratio = fuzz.ratio(orig_addr1.lower(), orig_addr2.lower())
                
                name_sim = fuzz.token_set_ratio(df.loc[id1, 'norm_name'], df.loc[id2, 'norm_name'])
                addr_sim = fuzz.token_set_ratio(df.loc[id1, 'norm_addr'], df.loc[id2, 'norm_addr'])
                
                # –õ–∏–±–µ—Ä–∞–ª—å–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ - —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –¥–æ–ª–∂–µ–Ω —Å—Ä–∞–±–æ—Ç–∞—Ç—å
                conditions = [
                    # –û—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏–µ –∏—Å—Ö–æ–¥–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
                    orig_name_ratio > 85 and orig_addr_ratio > 70,
                    # –•–æ—Ä–æ—à–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ
                    name_sim > 75 and addr_sim > 60,
                    # –û—Ç–ª–∏—á–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è, —Å—Ä–µ–¥–Ω–∏–µ –∞–¥—Ä–µ—Å–∞
                    name_sim > 90 and addr_sim > 40,
                    # –û—Ç–ª–∏—á–Ω—ã–µ –∞–¥—Ä–µ—Å–∞, —Å—Ä–µ–¥–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è
                    addr_sim > 90 and name_sim > 50,
                ]
                
                if any(conditions):
                    duplicates.add(frozenset([id1, id2]))
        
        return duplicates

    def get_duplicates_judge6_gentle_norm(self, df):
        """–°—É–¥—å—è 6: '–ú—è–≥–∫–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è' - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ–Ω–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞."""
        duplicates = set()
        ids = df.index.tolist()
        
        # –°–æ–∑–¥–∞–µ–º –º—è–≥–∫–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏
        gentle_names = {}
        gentle_addrs = {}
        
        for idx in ids:
            orig_name = str(df.loc[idx, 'orig_name']) if 'orig_name' in df.columns else ""
            orig_addr = str(df.loc[idx, 'orig_addr']) if 'orig_addr' in df.columns else ""
            
            gentle_names[idx] = self.normalize_text_gentle(orig_name)
            gentle_addrs[idx] = self.normalize_text_gentle(orig_addr)
        
        for i in range(len(ids)):
            for j in range(i + 1, len(ids)):
                id1, id2 = ids[i], ids[j]
                
                # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –º—è–≥–∫–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
                gentle_name_sim = fuzz.token_set_ratio(gentle_names[id1], gentle_names[id2])
                gentle_addr_sim = fuzz.token_set_ratio(gentle_addrs[id1], gentle_addrs[id2])
                
                # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ—Å—Ç–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫
                simple_name_sim = fuzz.ratio(gentle_names[id1], gentle_names[id2])
                simple_addr_sim = fuzz.ratio(gentle_addrs[id1], gentle_addrs[id2])
                
                # –ú—è–≥–∫–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏
                conditions = [
                    # –û—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏–µ –ø—Ä–∏ token_set —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏
                    gentle_name_sim > 80 and gentle_addr_sim > 70,
                    # –û—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏–µ –ø—Ä–∏ –ø—Ä–æ—Å—Ç–æ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏
                    simple_name_sim > 85 and simple_addr_sim > 75,
                    # –û—Ç–ª–∏—á–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è, —Å—Ä–µ–¥–Ω–∏–µ –∞–¥—Ä–µ—Å–∞
                    gentle_name_sim > 90 and gentle_addr_sim > 50,
                    # –û—Ç–ª–∏—á–Ω—ã–µ –∞–¥—Ä–µ—Å–∞, —Å—Ä–µ–¥–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è  
                    gentle_addr_sim > 90 and gentle_name_sim > 60,
                ]
                
                if any(conditions):
                    duplicates.add(frozenset([id1, id2]))
        
        return duplicates

    def find_duplicates(
        self,
        df: pd.DataFrame,
        name_column: str,
        address_column: Optional[str] = None,
        id_column: Optional[str] = None,
        min_votes: int = 2
    ) -> Tuple[List[List[int]], Dict]:
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–Ω—Å–∏–ª–∏—É–º–∞ –∏–∑ 4 —Å—É–¥–µ–π"""
        
        if df.empty:
            return [], {
                "total_records": 0,
                "duplicate_groups": 0,
                "duplicate_records": 0,
                "unique_records": 0,
            }

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df_work = df.copy()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        original_indices = df.index.tolist()
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∞ ID, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë, –∏–Ω–∞—á–µ —Å–æ–∑–¥–∞—ë–º —Å–≤–æ—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
        if id_column and id_column in df_work.columns:
            print(f"üîç ID –∫–æ–ª–æ–Ω–∫–∞: {id_column}")
            df_work['WorkId'] = original_indices
        else:
            df_work['WorkId'] = original_indices
        
        df_work = df_work.set_index('WorkId', drop=False)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è –ª–∏–±–µ—Ä–∞–ª—å–Ω–æ–≥–æ —Å—É–¥—å–∏
        df_work['orig_name'] = df_work[name_column].apply(lambda x: str(x) if pd.notna(x) else "")
        if address_column and address_column in df_work.columns:
            df_work['orig_addr'] = df_work[address_column].apply(lambda x: str(x) if pd.notna(x) else "")
        else:
            df_work['orig_addr'] = ""
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        df_work['norm_name'] = df_work[name_column].apply(lambda x: self.normalize_text(str(x) if pd.notna(x) else "", True))
        if address_column and address_column in df_work.columns:
            df_work['norm_addr'] = df_work[address_column].apply(lambda x: self.normalize_text(str(x) if pd.notna(x) else "", True))
        else:
            df_work['norm_addr'] = ""
        df_work['brand_name'] = df_work[name_column].apply(lambda x: self.normalize_brand_name(str(x) if pd.notna(x) else ""))
        
        # –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Å—É–¥–µ–π
        results1 = self.get_duplicates_judge1_strict(df_work)
        results2 = self.get_duplicates_judge2_geo(df_work)
        results3 = self.get_duplicates_judge3_brand(df_work)
        results4 = self.get_duplicates_judge4_weighted(df_work)
        results5 = self.get_duplicates_judge5_liberal(df_work)
        results6 = self.get_duplicates_judge6_gentle_norm(df_work)

        # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        print(f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—É–¥–µ–π:")
        print(f"   –°—É–¥—å—è 1 (–°—Ç—Ä–æ–≥–∏–π): {len(results1)} –ø–∞—Ä")
        print(f"   –°—É–¥—å—è 2 (–ì–µ–æ): {len(results2)} –ø–∞—Ä")
        print(f"   –°—É–¥—å—è 3 (–ë—Ä–µ–Ω–¥): {len(results3)} –ø–∞—Ä")
        print(f"   –°—É–¥—å—è 4 (–í–∑–≤–µ—à–µ–Ω–Ω—ã–π): {len(results4)} –ø–∞—Ä")
        print(f"   –°—É–¥—å—è 5 (–õ–∏–±–µ—Ä–∞–ª—å–Ω—ã–π): {len(results5)} –ø–∞—Ä")
        print(f"   –°—É–¥—å—è 6 (–ú—è–≥–∫–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è): {len(results6)} –ø–∞—Ä")

        # –ü–æ–¥—Å—á—ë—Ç –≥–æ–ª–æ—Å–æ–≤
        all_votes = Counter(results1)
        all_votes.update(results2)
        all_votes.update(results3)
        all_votes.update(results4)
        all_votes.update(results5)
        all_votes.update(results6)

        # –ê–Ω–∞–ª–∏–∑ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è
        vote_distribution = Counter(all_votes.values())
        print(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥–æ–ª–æ—Å–æ–≤:")
        for votes, count in sorted(vote_distribution.items()):
            print(f"   {votes} –≥–æ–ª–æ—Å(–æ–≤): {count} –ø–∞—Ä")

        # –ù–∞–π–¥—ë–º –ø–∞—Ä—ã, –Ω–∞–±—Ä–∞–≤—à–∏–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ—Å–æ–≤
        final_duplicate_pairs = [list(pair) for pair, count in all_votes.items() if count >= min_votes]
        print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã—Ö –ø–∞—Ä –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(final_duplicate_pairs)} (–º–∏–Ω. –≥–æ–ª–æ—Å–æ–≤: {min_votes})")
        print(f"üîç –ü–∞—Ä—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {final_duplicate_pairs}")

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º NetworkX
        G = nx.Graph()
        G.add_edges_from(final_duplicate_pairs)
        connected_components = list(nx.connected_components(G))
        final_groups = [list(group) for group in connected_components if len(group) > 1]
        print(f"üîç –ì—Ä—É–ø–ø—ã –ø–æ—Å–ª–µ NetworkX: {final_groups}")
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º ID –æ–±—Ä–∞—Ç–Ω–æ –≤ –∏–Ω–¥–µ–∫—Å—ã DataFrame
        # –ü–æ—Å–∫–æ–ª—å–∫—É –º—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –∫–∞–∫ WorkId, 
        # ID –≤ –≥—Ä—É–ø–ø–∞—Ö —É–∂–µ —è–≤–ª—è—é—Ç—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ DataFrame
        final_groups_indices = final_groups
        
        print(f"üîç –§–∏–Ω–∞–ª—å–Ω—ã–µ –≥—Ä—É–ø–ø—ã (–∏–Ω–¥–µ–∫—Å—ã): {final_groups_indices}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = {
            "total_records": len(df),
            "duplicate_groups": len(final_groups_indices),
            "duplicate_records": sum(len(group) for group in final_groups_indices),
            "unique_records": len(df) - sum(len(group) for group in final_groups_indices),
        }

        return final_groups_indices, stats

    def create_grouped_dataframe(self, df: pd.DataFrame, duplicate_groups: List[List[int]], id_column: Optional[str] = None) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ DataFrame —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        grouped_df = df.copy()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        grouped_df['Id —É–Ω–∏–∫–∞–ª—å–Ω–æ–π —Ç—Ç 2'] = ''
        
        print(f"üîç ID –∫–æ–ª–æ–Ω–∫–∞: {id_column}")
        
        # –î–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π ID –∏–∑ –≥—Ä—É–ø–ø—ã
        for group in duplicate_groups:
            print(f"üîç –ì—Ä—É–ø–ø–∞: {group}")
            if len(group) > 1:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –≥—Ä—É–ø–ø–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç–æ—è—â–∏–µ ID –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å—ã
                if id_column and id_column in grouped_df.columns:
                    group_ids = [grouped_df.iloc[idx][id_column] for idx in group if idx < len(grouped_df)]
                    min_id = min(group_ids) if group_ids else min(group)
                else:
                    min_id = min(group)
                
                for idx in group:
                    if idx < len(grouped_df):
                        grouped_df.iloc[idx, grouped_df.columns.get_loc('Id —É–Ω–∏–∫–∞–ª—å–Ω–æ–π —Ç—Ç 2')] = min_id
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞: —Å–Ω–∞—á–∞–ª–∞ –∑–∞–ø–∏—Å–∏ —Å –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏, –ø–æ—Ç–æ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ
        grouped_df['_sort_key'] = grouped_df['Id —É–Ω–∏–∫–∞–ª—å–Ω–æ–π —Ç—Ç 2'].apply(lambda x: 0 if x else 1)
        grouped_df = grouped_df.sort_values(['_sort_key', 'Id —É–Ω–∏–∫–∞–ª—å–Ω–æ–π —Ç—Ç 2'])
        grouped_df = grouped_df.drop('_sort_key', axis=1)
        
        return grouped_df

    def find_name_column(self, df: pd.DataFrame) -> Optional[str]:
        """–ü–æ–∏—Å–∫ –∫–æ–ª–æ–Ω–∫–∏ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏"""
        name_keywords = ['name', '–Ω–∞–∑–≤–∞–Ω–∏–µ', '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', 'title', 'company', '–∫–æ–º–ø–∞–Ω–∏—è', '—Ç—Ç']
        
        for col in df.columns:
            col_lower = col.lower()
            if any(keyword in col_lower for keyword in name_keywords):
                return col
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—É—é –∫–æ–ª–æ–Ω–∫—É
        return df.columns[0] if len(df.columns) > 0 else None

    def find_address_column(self, df: pd.DataFrame) -> Optional[str]:
        """–ü–æ–∏—Å–∫ –∫–æ–ª–æ–Ω–∫–∏ —Å –∞–¥—Ä–µ—Å–∞–º–∏"""
        address_keywords = ['address', '–∞–¥—Ä–µ—Å', 'location', '–º–µ—Å—Ç–æ', '—Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ']
        
        for col in df.columns:
            col_lower = col.lower()
            if any(keyword in col_lower for keyword in address_keywords):
                return col
        
        return None
