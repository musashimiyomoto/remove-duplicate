import pandas as pd
import re
from thefuzz import fuzz
import networkx as nx
from collections import Counter
import colorsys
from typing import List, Dict, Tuple, Optional
import warnings
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer

warnings.filterwarnings("ignore")


class DuplicateDetector:
    def __init__(self, similarity_threshold: float = 0.80):
        self.similarity_threshold = similarity_threshold
        self._setup_nltk()

    def _setup_nltk(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLTK —Ä–µ—Å—É—Ä—Å–æ–≤"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Ä—É—Å—Å–∫–∏—Ö —Å—Ç–æ–ø-—Å–ª–æ–≤
            self.russian_stopwords = set(stopwords.words('russian'))
            self.stemmer = SnowballStemmer('russian')
            self.use_nltk = True
        except Exception as e:
            print(f"‚ö†Ô∏è NLTK resources not available: {e}")
            self.russian_stopwords = set()
            self.stemmer = None
            self.use_nltk = False

    def generate_colors(
        self, num_colors: int, is_dark_theme: bool = False
    ) -> List[str]:
        colors = []
        for i in range(num_colors):
            hue = i / num_colors

            if is_dark_theme:
                saturation = 0.6 + (i % 3) * 0.1
                lightness = 0.4 + (i % 4) * 0.1
            else:
                saturation = 0.4 + (i % 3) * 0.1
                lightness = 0.85 + (i % 3) * 0.05

            rgb = colorsys.hls_to_rgb(hue, lightness, saturation)
            hex_color = "#{:02x}{:02x}{:02x}".format(
                int(rgb[0] * 255), int(rgb[1] * 255), int(rgb[2] * 255)
            )
            colors.append(hex_color)
        return colors

    def normalize_text(self, text, sort_words=True):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º NLTK"""
        if not isinstance(text, str): 
            return ""
            
        text = text.lower()
        
        # –ë–∞–∑–æ–≤—ã–µ –∑–∞–º–µ–Ω—ã –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        replacements = {
            r'\b–æ–±—â–µ—Å—Ç–≤–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å—é\b': '–æ–æ–æ', 
            r'(\b–∏–ø\b)|(\b–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å\b)': ' ',
            r'\b–≥–æ—Ä–æ–¥\b': '–≥', 
            r'\b—É–ª–∏—Ü–∞\b': '—É–ª', 
            r'\b–¥–æ–º\b': '–¥', 
            r'\b—Å—Ç—Ä–æ–µ–Ω–∏–µ\b': '—Å—Ç—Ä', 
            r'\b–∫–æ—Ä–ø—É—Å\b': '–∫',
        }
        for p, r in replacements.items(): 
            text = re.sub(p, r, text)
        
        if self.use_nltk:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º NLTK –¥–ª—è –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            try:
                # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é NLTK (–ª—É—á—à–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é)
                tokens = word_tokenize(text, language='russian')
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã –∏ —Ü–∏—Ñ—Ä—ã
                tokens = [token for token in tokens if re.match(r'^[a-z–∞-—è0-9]+$', token)]
                
                # –£–¥–∞–ª—è–µ–º —Ä—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
                tokens = [token for token in tokens if token not in self.russian_stopwords]
                
                # –°—Ç–µ–º–º–∏–Ω–≥ –¥–ª—è –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è –∫ –∫–æ—Ä–Ω–µ–≤–æ–π —Ñ–æ—Ä–º–µ
                if self.stemmer:
                    tokens = [self.stemmer.stem(token) for token in tokens]
                
                words = tokens
            except Exception as e:
                # Fallback –∫ –±–∞–∑–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ NLTK
                text = re.sub(r'[^a-z–∞-—è0-9\s]', ' ', text)
                words = text.split()
        else:
            # –ë–∞–∑–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ NLTK
            text = re.sub(r'[^a-z–∞-—è0-9\s]', ' ', text)
            words = text.split()
        
        # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ (–º–µ–Ω–µ–µ 2 —Å–∏–º–≤–æ–ª–æ–≤)
        words = [word for word in words if len(word.strip()) >= 2]
        
        if sort_words: 
            words = sorted(words)
        return ' '.join(words).strip()

    def normalize_brand_name(self, text):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –±—Ä–µ–Ω–¥–æ–≤–æ–≥–æ –∏–º–µ–Ω–∏ —Å NLTK"""
        base_normalized = self.normalize_text(text, sort_words=False)
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ç–∏–ø–æ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        generic_words = {
            '–æ–æ–æ', '–∫–∞—Ñ–µ', '–±–∞—Ä', '—Ä–µ—Å—Ç–æ—Ä–∞–Ω', '—Ñ–∞—Å—Ç—Ñ—É–¥', '–º–∞–≥–∞–∑–∏–Ω', '–ø–∏—Ü—Ü–µ—Ä–∏—è', 
            '–∫–ª—É–±', '–∫–æ—Ñ–µ–π–Ω—è', '—Å—Ç–æ–ª–æ–≤', '–µ–¥', '—Ç–æ—Ä–≥–æ–≤', '—Ü–µ–Ω—Ç—Ä', '–¥–æ–º', '–∫–æ–º–ø–∞–Ω',
            '–ø—Ä–µ–¥–ø—Ä–∏—è—Ç', '–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω', '–æ–±—â–µ—ç—Å—Ç–≤', '–æ–≥—Ä–∞–Ω–∏—á–µ–Ω', '–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç'
        }
        
        words = [word for word in base_normalized.split() if word not in generic_words]
        return ' '.join(sorted(words))

    # ==========================================================================
    # –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ß–ï–¢–´–†–ï–• –ù–ï–ó–ê–í–ò–°–ò–ú–´–• –°–£–î–ï–ô
    # ==========================================================================

    def get_duplicates_judge1_strict(self, df):
        """–°—É–¥—å—è 1: '–°—Ç—Ä–æ–≥–∏–π –ö–æ–Ω—Ç—Ä–æ–ª—å'."""
        duplicates = set()
        ids = df.index.tolist()
        for i in range(len(ids)):
            for j in range(i + 1, len(ids)):
                id1, id2 = ids[i], ids[j]
                name_sim = fuzz.token_set_ratio(df.loc[id1, 'norm_name'], df.loc[id2, 'norm_name'])
                addr_sim = fuzz.token_set_ratio(df.loc[id1, 'norm_addr'], df.loc[id2, 'norm_addr'])
                if addr_sim > 95 and name_sim > 90:
                    duplicates.add(frozenset([id1, id2]))
        return duplicates

    def get_duplicates_judge2_geo(self, df):
        """–°—É–¥—å—è 2: '–ì–µ–æ-–ê–Ω–∞–ª–∏—Ç–∏–∫'."""
        duplicates = set()
        ids = df.index.tolist()
        for i in range(len(ids)):
            for j in range(i + 1, len(ids)):
                id1, id2 = ids[i], ids[j]
                addr_sim = fuzz.token_set_ratio(df.loc[id1, 'norm_addr'], df.loc[id2, 'norm_addr'])
                if addr_sim > 97:
                    name_sim = fuzz.token_set_ratio(df.loc[id1, 'norm_name'], df.loc[id2, 'norm_name'])
                    if name_sim > 70:
                        duplicates.add(frozenset([id1, id2]))
        return duplicates

    def get_duplicates_judge3_brand(self, df):
        """–°—É–¥—å—è 3: '–ê–Ω–∞–ª–∏–∑ –ø–æ –ë—Ä–µ–Ω–¥—É'."""
        duplicates = set()
        ids = df.index.tolist()
        for i in range(len(ids)):
            for j in range(i + 1, len(ids)):
                id1, id2 = ids[i], ids[j]
                brand_name_sim = fuzz.token_set_ratio(df.loc[id1, 'brand_name'], df.loc[id2, 'brand_name'])
                if brand_name_sim > 95:
                    addr_sim = fuzz.token_set_ratio(df.loc[id1, 'norm_addr'], df.loc[id2, 'norm_addr'])
                    if addr_sim > 75:
                        duplicates.add(frozenset([id1, id2]))
        return duplicates

    def get_duplicates_judge4_weighted(self, df):
        """–°—É–¥—å—è 4: '–ò–Ω—Ç–µ–≥—Ä–∞—Ç–æ—Ä' (–í–∑–≤–µ—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞)."""
        duplicates = set()
        SIMILARITY_THRESHOLD = 88  # –ü–æ—Ä–æ–≥ –¥–ª—è –±–æ–ª—å—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
        NAME_WEIGHT = 0.45
        ADDRESS_WEIGHT = 0.55
        ids = df.index.tolist()
        for i in range(len(ids)):
            for j in range(i + 1, len(ids)):
                id1, id2 = ids[i], ids[j]
                name_sim = fuzz.token_set_ratio(df.loc[id1, 'norm_name'], df.loc[id2, 'norm_name'])
                addr_sim = fuzz.token_set_ratio(df.loc[id1, 'norm_addr'], df.loc[id2, 'norm_addr'])
                weighted_similarity = (name_sim * NAME_WEIGHT) + (addr_sim * ADDRESS_WEIGHT)
                if weighted_similarity >= SIMILARITY_THRESHOLD:
                    duplicates.add(frozenset([id1, id2]))
        return duplicates

    def find_duplicates(
        self,
        df: pd.DataFrame,
        name_column: str,
        address_column: Optional[str] = None,
        id_column: Optional[str] = None,
        min_votes: int = 2
    ) -> Tuple[List[List[int]], Dict]:
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–Ω—Å–∏–ª–∏—É–º–∞ –∏–∑ 4 —Å—É–¥–µ–π"""
        
        if df.empty:
            return [], {
                "total_records": 0,
                "duplicate_groups": 0,
                "duplicate_records": 0,
                "unique_records": 0,
            }

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df_work = df.copy()
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∞ ID, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë, –∏–Ω–∞—á–µ —Å–æ–∑–¥–∞—ë–º —Å–≤–æ—é
        if id_column and id_column in df_work.columns:
            df_work['Id'] = df_work[id_column]
        else:
            df_work['Id'] = range(len(df_work))
        
        df_work = df_work.set_index('Id', drop=False)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        df_work['norm_name'] = df_work[name_column].apply(lambda x: self.normalize_text(str(x) if pd.notna(x) else "", True))
        if address_column and address_column in df_work.columns:
            df_work['norm_addr'] = df_work[address_column].apply(lambda x: self.normalize_text(str(x) if pd.notna(x) else "", True))
        else:
            df_work['norm_addr'] = ""
        df_work['brand_name'] = df_work[name_column].apply(lambda x: self.normalize_brand_name(str(x) if pd.notna(x) else ""))
        
        # –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Å—É–¥–µ–π
        results1 = self.get_duplicates_judge1_strict(df_work)
        results2 = self.get_duplicates_judge2_geo(df_work)
        results3 = self.get_duplicates_judge3_brand(df_work)
        results4 = self.get_duplicates_judge4_weighted(df_work)

        # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        print(f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—É–¥–µ–π:")
        print(f"   –°—É–¥—å—è 1 (–°—Ç—Ä–æ–≥–∏–π): {len(results1)} –ø–∞—Ä")
        print(f"   –°—É–¥—å—è 2 (–ì–µ–æ): {len(results2)} –ø–∞—Ä")
        print(f"   –°—É–¥—å—è 3 (–ë—Ä–µ–Ω–¥): {len(results3)} –ø–∞—Ä")
        print(f"   –°—É–¥—å—è 4 (–í–∑–≤–µ—à–µ–Ω–Ω—ã–π): {len(results4)} –ø–∞—Ä")

        # –ü–æ–¥—Å—á—ë—Ç –≥–æ–ª–æ—Å–æ–≤
        all_votes = Counter(results1)
        all_votes.update(results2)
        all_votes.update(results3)
        all_votes.update(results4)

        # –ê–Ω–∞–ª–∏–∑ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è
        vote_distribution = Counter(all_votes.values())
        print(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥–æ–ª–æ—Å–æ–≤:")
        for votes, count in sorted(vote_distribution.items()):
            print(f"   {votes} –≥–æ–ª–æ—Å(–æ–≤): {count} –ø–∞—Ä")

        # –ù–∞–π–¥—ë–º –ø–∞—Ä—ã, –Ω–∞–±—Ä–∞–≤—à–∏–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ—Å–æ–≤
        final_duplicate_pairs = [list(pair) for pair, count in all_votes.items() if count >= min_votes]
        print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã—Ö –ø–∞—Ä –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(final_duplicate_pairs)} (–º–∏–Ω. –≥–æ–ª–æ—Å–æ–≤: {min_votes})")

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º NetworkX
        G = nx.Graph()
        G.add_edges_from(final_duplicate_pairs)
        connected_components = list(nx.connected_components(G))
        final_groups = [list(group) for group in connected_components if len(group) > 1]
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º ID –æ–±—Ä–∞—Ç–Ω–æ –≤ –∏–Ω–¥–µ–∫—Å—ã DataFrame
        final_groups_indices = []
        for group in final_groups:
            group_indices = []
            for id_val in group:
                # –ù–∞–π–¥—ë–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å –≤ –∏—Å—Ö–æ–¥–Ω–æ–º DataFrame
                matching_rows = df[df.index == id_val].index.tolist()
                if matching_rows:
                    group_indices.append(matching_rows[0])
                else:
                    # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ ID, –ø–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ –ø–æ–∑–∏—Ü–∏–∏
                    if isinstance(id_val, int) and id_val < len(df):
                        group_indices.append(id_val)
            if group_indices:
                final_groups_indices.append(group_indices)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = {
            "total_records": len(df),
            "duplicate_groups": len(final_groups_indices),
            "duplicate_records": sum(len(group) for group in final_groups_indices),
            "unique_records": len(df) - sum(len(group) for group in final_groups_indices),
        }

        return final_groups_indices, stats

    def create_grouped_dataframe(self, df: pd.DataFrame, duplicate_groups: List[List[int]]) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ DataFrame —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        grouped_df = df.copy()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        grouped_df['Id —É–Ω–∏–∫–∞–ª—å–Ω–æ–π —Ç—Ç 2'] = ''
        
        # –î–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π ID –∏–∑ –≥—Ä—É–ø–ø—ã
        for group_num, group in enumerate(duplicate_groups, 1):
            if len(group) > 1:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –≥—Ä—É–ø–ø–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                # –ù–∞—Ö–æ–¥–∏–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π ID –≤ –≥—Ä—É–ø–ø–µ
                min_id = min(group)
                for idx in group:
                    if idx < len(grouped_df):
                        grouped_df.iloc[idx, grouped_df.columns.get_loc('Id —É–Ω–∏–∫–∞–ª—å–Ω–æ–π —Ç—Ç 2')] = min_id
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞: —Å–Ω–∞—á–∞–ª–∞ –∑–∞–ø–∏—Å–∏ —Å –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏, –ø–æ—Ç–æ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ
        grouped_df['_sort_key'] = grouped_df['Id —É–Ω–∏–∫–∞–ª—å–Ω–æ–π —Ç—Ç 2'].apply(lambda x: 0 if x else 1)
        grouped_df = grouped_df.sort_values(['_sort_key', 'Id —É–Ω–∏–∫–∞–ª—å–Ω–æ–π —Ç—Ç 2'])
        grouped_df = grouped_df.drop('_sort_key', axis=1)
        
        return grouped_df

    def create_styled_dataframe(self, df: pd.DataFrame, duplicate_groups: List[List[int]], is_dark_theme: bool = False) -> Dict:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç–∏–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ DataFrame –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ"""
        if not duplicate_groups:
            return {
                "data": df.values.tolist(),
                "headers": df.columns.tolist(),
            }
        
        colors = self.generate_colors(len(duplicate_groups), is_dark_theme)
        
        # –°–æ–∑–¥–∞—ë–º –∫–∞—Ä—Ç—É —Ü–≤–µ—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏
        row_colors = [''] * len(df)
        for i, group in enumerate(duplicate_groups):
            for idx in group:
                if idx < len(df):
                    row_colors[idx] = colors[i % len(colors)]
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å —Ü–≤–µ—Ç–æ–≤–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π
        styled_data = []
        for i, row in enumerate(df.values.tolist()):
            if row_colors[i]:
                styled_row = [f"<div style='background-color: {row_colors[i]}; padding: 5px;'>{cell}</div>" for cell in row]
                styled_data.append(styled_row)
            else:
                styled_data.append(row)
        
        return {
            "data": styled_data,
            "headers": df.columns.tolist(),
        }

    def find_name_column(self, df: pd.DataFrame) -> Optional[str]:
        """–ü–æ–∏—Å–∫ –∫–æ–ª–æ–Ω–∫–∏ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏"""
        name_keywords = ['name', '–Ω–∞–∑–≤–∞–Ω–∏–µ', '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', 'title', 'company', '–∫–æ–º–ø–∞–Ω–∏—è', '—Ç—Ç']
        
        for col in df.columns:
            col_lower = col.lower()
            if any(keyword in col_lower for keyword in name_keywords):
                return col
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—É—é –∫–æ–ª–æ–Ω–∫—É
        return df.columns[0] if len(df.columns) > 0 else None

    def find_address_column(self, df: pd.DataFrame) -> Optional[str]:
        """–ü–æ–∏—Å–∫ –∫–æ–ª–æ–Ω–∫–∏ —Å –∞–¥—Ä–µ—Å–∞–º–∏"""
        address_keywords = ['address', '–∞–¥—Ä–µ—Å', 'location', '–º–µ—Å—Ç–æ', '—Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ']
        
        for col in df.columns:
            col_lower = col.lower()
            if any(keyword in col_lower for keyword in address_keywords):
                return col
        
        return None
